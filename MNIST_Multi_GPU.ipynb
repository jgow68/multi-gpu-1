{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Part 3: Introduction </h1>\n",
    "\n",
    "<div> The goal of this lab is threefold:\n",
    "    <ul>\n",
    "        <li> To make minimal amount of changes to our existing code to implement multi GPU implementation.</li>\n",
    "        <li> Reiterate the basic principles involved in training of a neural network this time applied to a multi GPU example. </li>\n",
    "        <li> Illustrate how increasing the number of GPUs affects the batch size and the training process behavior</li>\n",
    "    </ul>\n",
    "</div>\n",
    "<br/>\n",
    "<div>As discussed in the lecture we will modify our neural network implementation by:\n",
    "<ul>\n",
    "<li>Making a copy of our model and pinning each to a different GPU</li>\n",
    "<li>Implementing a mechanism through which gradients computed by each of the copies can be averaged (which is the essence of the Data Parallel implementation of Stochastic Gradient Descent).</li>\n",
    "</ul>\n",
    "\n",
    "    \n",
    "<br/> This code example implements an approach referred to as in-graph replication. This means that we have just a single process and a single execution graph.  Parts of this execution graph (the same copies of the model) are distributed across multiple GPUs. In our case we have made further simplifications by forcing model parameters (so W and B) to be not only shared between the model copies but also hosted in the computer memory. In this case the host memory becomes our “Parameter Server”. This approach is the simplest implementation of the Model Parallelism possible in TensorFlow. In practice it is rarely used as it introduces significant load on both the PCIe bus as well as the rest of the host system. \n",
    "<br/><br/>\n",
    "<h3> A side note</h3>\n",
    "A much more common (and substantially more efficient approach) is referred to as between-graph replication. In this approach every copy of the graph is managed by a separate process and each of the processes will have its own TensorFlow session and full copy of both the graph and model parameters. The communication between GPUs can be handled in a unique way, either through the TensorFlow communication mechanism or preferably using the NVIDIA Collective Communication Library (NCCL). NCCL is a very efficient library that allows multiple GPU, within the same system (using NVLINK) or within the same network to exchange the content of their memory directly without engaging the CPU or other computer resources. We will discuss the challenges involved in exchanging parameters as well as a substantially more efficient approach in more detail in Lecture and Lab 2 which discusses engineering challenges of scale.\n",
    "</div>\n",
    "\n",
    "<h1> Code review </h1>\n",
    "<div> Lets step through the code together and discuss changes we have made to the neural network implemented in the previous lab.   </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "# Suppress any unnecessary messages to make the output cleaner\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>We will use this variable to decide how many GPUs we want to assign to this process. Bear in mind that this virtual machine has finite amount of resources.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define how many GPUs we want to use\n",
    "num_gpus = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>Once again we define the hyperparameters of our model and the learning process</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "num_steps = 5000\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>Since we will have more than one device working on our problem we need to implement a mechanism to keep track of the progress</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable to count the number of train() calls. This equals the\n",
    "# number of batches processed * FLAGS.num_gpus.\n",
    "global_step = tf.get_variable(\n",
    "    'global_step', [],\n",
    "    initializer=tf.constant_initializer(0), trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>Since we will be creating multiple copies of the model it makes sense to define our model in a function that can be called multiple times (once per GPU). Since we will be sharing model parameters between the copies of the model we do not define W and B here but we pass them as the parameters of our function. By convention a copy of a model deployed on a GPU is referred to as Tower (because our models are very deep/tall and the resulting graph resembles multiple towers).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since every GPU will receive a copy of the same model (tower) we will define a function for model creation\n",
    "# which will help us deploy it across the devices\n",
    "def tower(W,B):\n",
    "    # Create the placeholders for the data to be used in TensorFlow\n",
    "    X = tf.placeholder(\"float\", [None, num_input], name=\"X\")\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes], name=\"Y\")\n",
    "    \n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(X, W['h1']), B['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, W['h2']), B['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, W['out']) + B['out']\n",
    "\n",
    "    # And a softmax to give us the actual prediction\n",
    "    prediction = tf.nn.softmax(out_layer)\n",
    "    \n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name = 'accuracy')\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=out_layer, labels=Y), name = 'loss')\n",
    "    \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once again we use the Adam optimiser\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>In this section we define shared model parameters. We then iterate over our GPUs and create a copy of our model on every device.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tower_grads = []\n",
    "loss_arr = []\n",
    "acc_arr = []\n",
    "\n",
    "# We create a shared copy of the model parameters in the host memory (we are not enforcing GPU placement)\n",
    "with tf.variable_scope(\"sharedParameters\",reuse=True) as scope:\n",
    "    W = {\n",
    "        'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "    }\n",
    "    B = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "    }\n",
    "    \n",
    "for i in range(num_gpus):\n",
    "    with tf.device('/device:GPU:%d' % i):\n",
    "        scopeName = '%s_%d' % (\"tower\", i)\n",
    "        with tf.variable_scope(scopeName,reuse=True) as scope:\n",
    "            # We create a copy of our model for every GPU and prefix all of the parameters\n",
    "            # with a scope indicating GPU placement (tower_0, tower_1, etc.).\n",
    "            loss, accuracy = tower(W,B)\n",
    "            loss_arr.append(loss)\n",
    "            acc_arr.append(accuracy)\n",
    "            # We want to calculate the gradient of the loss with respect to all variables\n",
    "            # but only the variables in this scope (so in this tower)\n",
    "            grads = optimizer.compute_gradients(loss)#,\n",
    "                                                #var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scopeName))\n",
    "            \n",
    "            # We store all of the calculated gradients for all towers in an array so they can be aggregated\n",
    "            tower_grads.append(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once again we define variables that will allow us to monitor our training process.\n",
    "total_loss = tf.reduce_mean(loss_arr)\n",
    "total_acc = tf.reduce_mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>Since now we have multiple copies of the model and every copy will be exposed to a different mini batch they will produce different values for the gradient. In this section we take those different results and average them. This effectively is equivalent to doubling our batch size. Through the averaging process we reduce the amount of noise in the gradient calculation. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor 'Mean_2:0' shape=(784, 256) dtype=float32>,\n",
       "  <tf.Variable 'sharedParameters/Variable:0' shape=(784, 256) dtype=float32_ref>),\n",
       " (<tf.Tensor 'Mean_3:0' shape=(256, 256) dtype=float32>,\n",
       "  <tf.Variable 'sharedParameters/Variable_1:0' shape=(256, 256) dtype=float32_ref>),\n",
       " (<tf.Tensor 'Mean_4:0' shape=(256, 10) dtype=float32>,\n",
       "  <tf.Variable 'sharedParameters/Variable_2:0' shape=(256, 10) dtype=float32_ref>),\n",
       " (<tf.Tensor 'Mean_5:0' shape=(256,) dtype=float32>,\n",
       "  <tf.Variable 'sharedParameters/Variable_3:0' shape=(256,) dtype=float32_ref>),\n",
       " (<tf.Tensor 'Mean_6:0' shape=(256,) dtype=float32>,\n",
       "  <tf.Variable 'sharedParameters/Variable_4:0' shape=(256,) dtype=float32_ref>),\n",
       " (<tf.Tensor 'Mean_7:0' shape=(10,) dtype=float32>,\n",
       "  <tf.Variable 'sharedParameters/Variable_5:0' shape=(10,) dtype=float32_ref>)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(<tf.Tensor 'tower_0/gradients/tower_0/MatMul_grad/tuple/control_dependency_1:0' shape=(784, 256) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable:0' shape=(784, 256) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_0/gradients/tower_0/MatMul_1_grad/tuple/control_dependency_1:0' shape=(256, 256) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_1:0' shape=(256, 256) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_0/gradients/tower_0/MatMul_2_grad/tuple/control_dependency_1:0' shape=(256, 10) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_2:0' shape=(256, 10) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_0/gradients/tower_0/Add_grad/tuple/control_dependency_1:0' shape=(256,) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_3:0' shape=(256,) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_0/gradients/tower_0/Add_1_grad/tuple/control_dependency_1:0' shape=(256,) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_4:0' shape=(256,) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_0/gradients/tower_0/add_grad/tuple/control_dependency_1:0' shape=(10,) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_5:0' shape=(10,) dtype=float32_ref>)],\n",
       " [(<tf.Tensor 'tower_1/gradients/tower_1/MatMul_grad/tuple/control_dependency_1:0' shape=(784, 256) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable:0' shape=(784, 256) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_1/gradients/tower_1/MatMul_1_grad/tuple/control_dependency_1:0' shape=(256, 256) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_1:0' shape=(256, 256) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_1/gradients/tower_1/MatMul_2_grad/tuple/control_dependency_1:0' shape=(256, 10) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_2:0' shape=(256, 10) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_1/gradients/tower_1/Add_grad/tuple/control_dependency_1:0' shape=(256,) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_3:0' shape=(256,) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_1/gradients/tower_1/Add_1_grad/tuple/control_dependency_1:0' shape=(256,) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_4:0' shape=(256,) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_1/gradients/tower_1/add_grad/tuple/control_dependency_1:0' shape=(10,) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_5:0' shape=(10,) dtype=float32_ref>)],\n",
       " [(<tf.Tensor 'tower_2/gradients/tower_2/MatMul_grad/tuple/control_dependency_1:0' shape=(784, 256) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable:0' shape=(784, 256) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_2/gradients/tower_2/MatMul_1_grad/tuple/control_dependency_1:0' shape=(256, 256) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_1:0' shape=(256, 256) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_2/gradients/tower_2/MatMul_2_grad/tuple/control_dependency_1:0' shape=(256, 10) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_2:0' shape=(256, 10) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_2/gradients/tower_2/Add_grad/tuple/control_dependency_1:0' shape=(256,) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_3:0' shape=(256,) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_2/gradients/tower_2/Add_1_grad/tuple/control_dependency_1:0' shape=(256,) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_4:0' shape=(256,) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_2/gradients/tower_2/add_grad/tuple/control_dependency_1:0' shape=(10,) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_5:0' shape=(10,) dtype=float32_ref>)],\n",
       " [(<tf.Tensor 'tower_3/gradients/tower_3/MatMul_grad/tuple/control_dependency_1:0' shape=(784, 256) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable:0' shape=(784, 256) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_3/gradients/tower_3/MatMul_1_grad/tuple/control_dependency_1:0' shape=(256, 256) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_1:0' shape=(256, 256) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_3/gradients/tower_3/MatMul_2_grad/tuple/control_dependency_1:0' shape=(256, 10) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_2:0' shape=(256, 10) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_3/gradients/tower_3/Add_grad/tuple/control_dependency_1:0' shape=(256,) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_3:0' shape=(256,) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_3/gradients/tower_3/Add_1_grad/tuple/control_dependency_1:0' shape=(256,) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_4:0' shape=(256,) dtype=float32_ref>),\n",
       "  (<tf.Tensor 'tower_3/gradients/tower_3/add_grad/tuple/control_dependency_1:0' shape=(10,) dtype=float32>,\n",
       "   <tf.Variable 'sharedParameters/Variable_5:0' shape=(10,) dtype=float32_ref>)]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tower_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "    \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
    "    Note that this function provides a synchronization point across all towers.\n",
    "        Args:\n",
    "        tower_grads: List of lists of (gradient, variable) tuples. The outer list\n",
    "                is over individual gradients. The inner list is over the gradient\n",
    "                calculation for each tower.\n",
    "        Returns:\n",
    "        List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "        across all towers.\n",
    "    \"\"\"\n",
    "    \n",
    "    average_grads = []\n",
    "    \n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(grads,0)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must calculate the mean of each gradient. Note that this is the\n",
    "# synchronization point across all towers.\n",
    "grads = average_gradients(tower_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>Now that we have an averaged gradient we use it to execute the optimisation step.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = optimizer.apply_gradients(grads, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>This is just a helper function that prepares the minibatch.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeedDict(num_gpus, batch_size):\n",
    "    feed_dict = {}\n",
    "    for i in range(num_gpus):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        scopeName = '%s_%d' % (\"tower\", i)\n",
    "        feed_dict[scopeName+ \"/X:0\"] = batch_x\n",
    "        feed_dict[scopeName+ \"/Y:0\"] = batch_y\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_array = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>Now that we have defined the model, implemented gradient averaging and prepared the data for the multi gpu training the rest of the process is fairly identical to what we saw in the previous exercise.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 7277.1758, Accuracy= 0.5625\n",
      "Step 100, Minibatch Loss= 130.6741, Accuracy= 0.8906\n",
      "Step 200, Minibatch Loss= 66.4323, Accuracy= 0.8828\n",
      "Step 300, Minibatch Loss= 31.8328, Accuracy= 0.9375\n",
      "Step 400, Minibatch Loss= 30.2908, Accuracy= 0.8750\n",
      "Step 500, Minibatch Loss= 26.9680, Accuracy= 0.8906\n",
      "Step 600, Minibatch Loss= 24.2311, Accuracy= 0.9062\n",
      "Step 700, Minibatch Loss= 18.6471, Accuracy= 0.8594\n",
      "Step 800, Minibatch Loss= 13.9185, Accuracy= 0.8906\n",
      "Step 900, Minibatch Loss= 10.2085, Accuracy= 0.9141\n",
      "Step 1000, Minibatch Loss= 9.6815, Accuracy= 0.9062\n",
      "Step 1100, Minibatch Loss= 11.6505, Accuracy= 0.8906\n",
      "Step 1200, Minibatch Loss= 7.5206, Accuracy= 0.9141\n",
      "Step 1300, Minibatch Loss= 8.3156, Accuracy= 0.8750\n",
      "Step 1400, Minibatch Loss= 9.4502, Accuracy= 0.8672\n",
      "Step 1500, Minibatch Loss= 8.7125, Accuracy= 0.8906\n",
      "Step 1600, Minibatch Loss= 8.1995, Accuracy= 0.9531\n",
      "Step 1700, Minibatch Loss= 9.1531, Accuracy= 0.9219\n",
      "Step 1800, Minibatch Loss= 10.5440, Accuracy= 0.8438\n",
      "Step 1900, Minibatch Loss= 8.8646, Accuracy= 0.9062\n",
      "Step 2000, Minibatch Loss= 10.4892, Accuracy= 0.8516\n",
      "Step 2100, Minibatch Loss= 10.0170, Accuracy= 0.8672\n",
      "Step 2200, Minibatch Loss= 14.0702, Accuracy= 0.8906\n",
      "Step 2300, Minibatch Loss= 12.8275, Accuracy= 0.8672\n",
      "Step 2400, Minibatch Loss= 10.5840, Accuracy= 0.8984\n",
      "Step 2500, Minibatch Loss= 19.9100, Accuracy= 0.8672\n",
      "Step 2600, Minibatch Loss= 7.6211, Accuracy= 0.9375\n",
      "Step 2700, Minibatch Loss= 10.8090, Accuracy= 0.8672\n",
      "Step 2800, Minibatch Loss= 7.8297, Accuracy= 0.8828\n",
      "Step 2900, Minibatch Loss= 12.4964, Accuracy= 0.8438\n",
      "Step 3000, Minibatch Loss= 8.5424, Accuracy= 0.8594\n",
      "Step 3100, Minibatch Loss= 6.6567, Accuracy= 0.8281\n",
      "Step 3200, Minibatch Loss= 10.4232, Accuracy= 0.8516\n",
      "Step 3300, Minibatch Loss= 15.7047, Accuracy= 0.8750\n",
      "Step 3400, Minibatch Loss= 14.9815, Accuracy= 0.8906\n",
      "Step 3500, Minibatch Loss= 7.5237, Accuracy= 0.8672\n",
      "Step 3600, Minibatch Loss= 5.6792, Accuracy= 0.9219\n",
      "Step 3700, Minibatch Loss= 4.6288, Accuracy= 0.8906\n",
      "Step 3800, Minibatch Loss= 4.1845, Accuracy= 0.8594\n",
      "Step 3900, Minibatch Loss= 5.8441, Accuracy= 0.8516\n",
      "Step 4000, Minibatch Loss= 4.1495, Accuracy= 0.9062\n",
      "Step 4100, Minibatch Loss= 5.0353, Accuracy= 0.8516\n",
      "Step 4200, Minibatch Loss= 3.4307, Accuracy= 0.9219\n",
      "Step 4300, Minibatch Loss= 3.4907, Accuracy= 0.8750\n",
      "Step 4400, Minibatch Loss= 4.4765, Accuracy= 0.7969\n",
      "Step 4500, Minibatch Loss= 2.2381, Accuracy= 0.8594\n",
      "Step 4600, Minibatch Loss= 1.8552, Accuracy= 0.8906\n",
      "Step 4700, Minibatch Loss= 1.6396, Accuracy= 0.9062\n",
      "Step 4800, Minibatch Loss= 1.6055, Accuracy= 0.8359\n",
      "Step 4900, Minibatch Loss= 1.5482, Accuracy= 0.8828\n",
      "Step 5000, Minibatch Loss= 1.6673, Accuracy= 0.8828\n",
      "Training Finished!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # We initialize the necessary variables.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        # Run optimization op (backprop)\n",
    "        feed_dict_val = getFeedDict(num_gpus, batch_size)\n",
    "        sess.run(train_op, feed_dict = feed_dict_val)\n",
    "        \n",
    "        # Calculate batch loss and accuracy\n",
    "        loss_val = sess.run(total_loss, feed_dict=feed_dict_val)\n",
    "        acc_val = sess.run(total_acc, feed_dict=feed_dict_val)\n",
    "        loss_array.append(loss_val)\n",
    "            \n",
    "        if step % display_step == 0 or step == 1:\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss_val) + \", Accuracy= \" + \"{:.4f}\".format(acc_val))\n",
    "            \n",
    "    print(\"Training Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>Now compare the shape of the below curve with the one we have produced in the previous exercise at the same batch size per GPU - 128</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_array)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
